
import streamlit as st
from card_rag import search_card
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv

load_dotenv()


# ================================ session_state ì„¤ì • ================================

# ì¼ë°˜ì ì¸ ì½”ë“œì—ì„œëŠ” memory ê°ì²´ë¥¼ ìƒì„±í•˜ë©´ ëŒ€í™” ë‚´ìš©ë“¤ì„ ê¸°ì–µí•˜ì§€ë§Œ, streamlitì—ì„œëŠ” ì›¹ ì„œë²„ì—ì„œ ìš”ì²­, ì‘ë‹µì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— 
# ì„¸ì…˜ì— ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ ì´ˆê¸°í™” ë¨(ë”°ë¼ì„œ memory ê°ì²´ë¥¼ session_stateì— ì €ì¥í•´ì•¼ í•¨)
if "pre_memory" not in st.session_state: 
    st.session_state["pre_memory"] = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

# í™”ë©´ì— ì¶œë ¥í•  ëŒ€í™” ê¸°ë¡ ì €ì¥: ChatGPT ì„œë¹„ìŠ¤ì™€ ìœ ì‚¬í•˜ê²Œ ì›¹ ìƒì—ì„œ ìš°ë¦¬ì˜ ì§ˆì˜ ì‘ë‹µ ë‚´ì—­ì´ ê³„ì† ë³´ì—¬ì ¸ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì„¸ì…˜ìœ¼ë¡œ ê´€ë¦¬ê°€ í•„ìš” 
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” ì¹´ë“œ ì¶”ì²œ AI Assistant ì…ë‹ˆë‹¤."}
    ]
    
# ================================ model & prompt ì„¤ì • ================================

# model ê°ì²´ ì •ì˜
model = ChatOpenAI(model="gpt-40-mini", temperature=0)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±: ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ aiì˜ ì‘ë‹µì„ ìœ ë„
system_prompt = """
ë„ˆëŠ” ì¹´ë“œì‚¬ ì§ì›ì´ì•¼. ê³ ê°ì˜ ì§ˆì˜ê°€ ë“¤ì–´ì˜¤ë©´ contextì— ë”°ë¼ ê°€ì¥ í˜œíƒì´ ë§ì€ ì¹´ë“œë¥¼ 3ê°œ ì¶”ì²œí•´ì¤˜. 
context ë‚´ìš©ì— í•œí•´ì„œë§Œ ì¶”ì²œí•´ì£¼ë˜, contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°œì„¤í•˜ì§€ ë§ì•„ì¤˜. 
contextë¥¼ ì°¸ê³ í•œ ì¶œë ¥ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ì•„.

--ì¶œë ¥ í¬ë§·--
ğŸ“Œ í•´ë‹¹ë€ì— ë¨¼ì € ì‚¬ìš©ìê°€ ì–´ë–¤ ì¹´ë“œë¥¼ ì›í•˜ëŠ”ì§€ íŒŒì•…í•´ì„œ ìš”ì•½ë³¸ì„ í•œ ì¤„ë¡œ ì‘ì„±í•´ì¤˜.
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
ğŸ’³ ì¶”ì²œì¹´ë“œëª…
    - ì¶”ì²œ ì´ìœ 
    - í•´ë‹¹ ì¹´ë“œì˜ í˜œíƒ
"""

user_prompt = """\
ì•„ë˜ì˜ ì‚¬ìš©ì questionì„ ì½ê³  contextë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´ë“œ(ì‚¬ìš©ìê°€ í˜œíƒì„ ìµœëŒ€ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹´ë“œ)ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

--chat_history--
{chat_history}

--question--
{question}

--context--
{context}
"""

final_prompt = ChatPromptTemplate({
    ("system", system_prompt),
    ("user", user_prompt)
})

# ì‚¬ìš©ì ì…ë ¥ê°’ì„ ë°›ì•„ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def get_user_input(question):
    return {
        "chat_history": st.session_state["pre_memory"].chat_memory.messages,
        "question": question,
        "context": search_card(question)
    }

chain = RunnableLambda(get_user_input) | final_prompt | model | StrOutputParser()

# ëŒ€í™” ë‚´ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡í•´ì£¼ëŠ” í•¨ìˆ˜ ì •ì˜
def conversation_with_memory(question):
    # 1. ë©”ì‹œì§€ ì¶œë ¥ ê³µê°„ ìƒì„±
    stream_placeholder = st.empty()
    
    # 2. ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
    full_response = ""
    for chunk in chain.stream(question):
        full_response += chunk
        stream_placeholder.write(full_response)
        
    # 3. ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ ai ì‘ë‹µì„ memoryì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
    st.session_state["pre_memory"].save_context(
        {"input": question},
        {"output": full_response}
    )

    # 4. session_state["messages"]ì— ì €ì¥í•  ìš©ë„ë¡œ full_response ë°˜í™˜
    return full_response

# ================================ ë©”ì¸í™”ë©´ ì„¤ì • ================================
st.title("My GPT")

# 1. ëŒ€í™” ê¸°ë¡ ì¶œë ¥
# ë°˜ë³µë¬¸ìœ¼ë¡œ messagesì— ìˆëŠ” ëª¨ë“  ëŒ€í™” ê¸°ë¡ì— ì ‘ê·¼
for message in st.session_state["messages"]:
    # chat_message: ë©”ì‹œì§€ì˜ ë°œì‹ ì role(assistantì¸ì§€ userì¸ì§€)ì— ë”°ë¼ UIë¥¼ êµ¬ë¶„í•˜ì—¬ ë©”ì‹œì§€ ì°½ì„ í‘œì‹œí•´ì£¼ëŠ” í•¨ìˆ˜ 
    with st.chat_message(message["role"]):  # ì—­í•  ì§€ì •
        st.write(message["content"]) # í•´ë‹¹ì—­í• ì˜ ë©”ì‹œì§€ ì¶œë ¥
# 2. ì‚¬ìš©ì ì§ˆì˜ ì‘ì„±
question = st.chat_input("ì‚¬ìš©ì ì…ë ¥")

# 3. ì‚¬ìš©ì ì§ˆì˜ ì €ì¥&ì¶œë ¥
if question:
    # ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ì˜ messageì— ì¶”ê°€
    st.session_state["messages"].append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.write(question)
        
# 4. AI ë‹µë³€ ìƒì„± & ì¶œë ¥
if st.session_state["messages"][-1]["role"] != "assistant": # message ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸´ ë©”ì‹œì§€ê°€ aiê°€ ì•„ë‹Œ ê²½ìš°
    with st.chat_message("assistant"):
        try:
            ai_response = conversation_with_memory(question)
            st.session_state["messages"].append({"role": "assistant", "content": ai_response})
            
        except Exception as e: 
            error_ = f"""\
ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.KeyError

ë°œìƒ ì—ëŸ¬: {e}
"""
            st.error(error_)